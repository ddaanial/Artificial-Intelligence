{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t\n",
    "<p></p>\n",
    "<p></p>\n",
    "<font size=5>\n",
    "In the Name of God\n",
    "<font/>\n",
    " <p></p>\n",
    " <br/>\n",
    "\t\t<div align=center>\n",
    "\t\t    <size=3.5>\n",
    "\t\t\t    <br />\n",
    "Practical Assignment 4- part 2\n",
    "              <font  size=4>\n",
    "            \t<br/>\n",
    "              N-grams\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    <br/>\n",
    " </div>\n",
    "<hr/>\n",
    "Artifical Intelligence - Dr. GholamReza GhassemSani\n",
    "</font>\n",
    "  <p></p>\n",
    " <br/>\n",
    "Sharif University of Technology\n",
    "<p></p>\n",
    "Spring 2022\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<font size=4 color=red>\n",
    " </font>\n",
    "                <br/>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assignment Details\n",
    "**Due date: 21/4/1401 (at 11:59pm)**<br>\n",
    "You are free to collaborate but solutions must be written up individually.\n",
    "Collaborators **must** be acknowledged.<br>\n",
    "Submissions with more than 100 hours delay will not be graded.<br>Submissions with less than\n",
    "50 hours delay will be penalized by the following rule:<br>\n",
    "**Penalized mark = M * (100 â€“ D) / 100** <br>\n",
    "Where M = the mark achieved from your solution and D is number of hours passed the\n",
    "deadline. Submissions within 50 < X â‰¤ 100 hours delay will be penalized by P.M. = M * 0.5.<br>\n",
    "Submit your answers on quera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "student_number = 400211546\n",
    " Name = danial\n",
    " Last_Name = ahangarani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this assignment you will be implementing N-gram language models. N-gram language models assume that each word $wi$\n",
    "depends only on the Nâˆ’1 preceding words. you will be using these language models for:\n",
    "*   calculating the probability of sentences\n",
    "*   predicting next word of a query\n",
    "\n",
    "###  Dataset\n",
    "Download dataset from [here](https://drive.google.com/drive/folders/1OOjVMG61mrBmGDh2KgPrO0HqrMPj-M0j?usp=sharing).  \n",
    "The dataset you will be working with for this assignment are available as `train.pickle` and `test.pickle` files. Each file includes a list of lists and each list contains tokens of a sentences/sentences.  \n",
    " Run the codes below to know more about the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train.pickle', 'rb') as handle:\n",
    "    train_data = pickle.load(handle)\n",
    "\n",
    "with open('test.pickle', 'rb') as handle:\n",
    "    test_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we have a sample tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chillin',\n",
       " 'on',\n",
       " 'it',\n",
       " 'waiting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'fam',\n",
       " 'to',\n",
       " 'get',\n",
       " 'in',\n",
       " 'wats',\n",
       " 'good',\n",
       " 'wit',\n",
       " 'u']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 1 (Preprocessing)\n",
    "At first explain what other preprocessing does the data need and and why then implement your suggested preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain your answers here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "#i converted each tokent in each sentence to lowercase\n",
    "# and removed some punctuation marks. \n",
    "import re\n",
    "train_data = [[token.lower() for token in string]for string in train_data]\n",
    "punc = ['#','$','%','&','(',')','*','+','-','/','<','=','>','@']\n",
    "train_data = [[token for token in string if token not in punc]for string in train_data]\n",
    "#train_data = [[re.sub(r'[^\\w\\s]','',token) for token in string ]for string in train_data]\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (Counting N-grams)\n",
    "Fill out the following code block to count the unigrams, bigrams and trigrams in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_ngrams(xs, n):\n",
    "    return [xs[i:i+n] for i in range(len(xs)-n+1)]\n",
    "def LanguageModel(data) -> dict:\n",
    "    ### you need to use your preprocessed data and count the unigram,bigram and trigrams \n",
    "    \n",
    "    ### Begin your code\n",
    "    unigram_counts = {}\n",
    "    for sen in data:\n",
    "        for word in sen:\n",
    "            if word not in unigram_counts:\n",
    "                unigram_counts[word] = 1\n",
    "            else:\n",
    "                unigram_counts[word] += 1\n",
    "     \n",
    "    bigram_counts = {}\n",
    "    for sen in data:\n",
    "        for bi in seq_ngrams(sen, 2):\n",
    "            if ' '.join(bi) not in bigram_counts:\n",
    "                bigram_counts[' '.join(bi)] = 1\n",
    "            else:\n",
    "                bigram_counts[' '.join(bi)] += 1\n",
    "                \n",
    "    trigram_counts = {}\n",
    "    for sen in data:\n",
    "        for tr in seq_ngrams(sen, 3):\n",
    "            if ' '.join(tr) not in trigram_counts:\n",
    "                trigram_counts[' '.join(tr)] = 1\n",
    "            else:\n",
    "                trigram_counts[' '.join(tr)] += 1\n",
    "    ### End your code\n",
    "     \n",
    "    ### return dictinaries with n-grams as keys and counts of n-grams as values\n",
    "    return unigram_counts, bigram_counts, trigram_counts\n",
    "\n",
    "unigram_counts, bigram_counts, trigram_counts = LanguageModel(train_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (Laplace Smoothing)\n",
    "In natural language:\n",
    "*   A small number of events (e.g. words) occur with high frequency\n",
    "*   large number of events occur with very low frequency\\\n",
    "So we canâ€™t actually evaluate our MLE models on\n",
    "unseen test data because both are likely to contain words/n-grams that these models assign zero probability to. In order to handle this problem we can use **1(laplace)smoothing** .  \n",
    "Fill out the following code cell to calculate the probability of unigram, bigram and trigram.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025685391064384226\n",
      "0.0012942053840104698\n",
      "0.00020255988727101927\n"
     ]
    }
   ],
   "source": [
    "# do not forget to add 1(laplace)smoothing or othe methods of smoothing(which you need to explain in another markdown cell)\n",
    "def calc_unigram_p(w_1): # p(w_1)\n",
    "    ### Begin your code\n",
    "    p = (unigram_counts.get(w_1,0) + 1)/(len(v) + V)\n",
    "    ### End your code\n",
    "    return p\n",
    "    \n",
    "def calc_bigram_p(w_1,w_2): # p(w_2|w_1)\n",
    "    ### Begin your code\n",
    "    \n",
    "    p = (bigram_counts.get(' '.join([w_1,w_2]),0) + 1)/(unigram_counts.get(w_1,0) + V)\n",
    "    ### End your code\n",
    "    return p\n",
    "\n",
    "def calc_trigram_p(w_1,w_2,w_3): # p(w_3|w_2,w_1)\n",
    "    ### Begin your code\n",
    "    p = (trigram_counts.get(' '.join([w_1,w_2, w_3]),0) + 1)/(bigram_counts.get(' '.join([w_1,w_2]),0) + V)\n",
    "    ### End your code\n",
    "    return p\n",
    "\n",
    "#claculate V for laplace formula.\n",
    "v = []\n",
    "for sen in train_data:\n",
    "    for word in sen:\n",
    "        v.append(word)\n",
    "V = len(set(v))\n",
    "\n",
    "print(calc_unigram_p('the'))\n",
    "print(calc_bigram_p('waiting', 'on'))\n",
    "print(calc_trigram_p('waiting', 'on', 'the'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (Probability of sentences)\n",
    "At this part first you need to choose 2 sentence(with more than 4 words) from the test data and  calculate the probability of the chosen sentences under each language model(unigram,bigram and trigram) as expplained below:\n",
    "  \n",
    "We will treat each sentence as a sequence of terms $(w_1, \\ldots, w_n)$ whose probability under unigram model is computed as :\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2)\\cdots P(w_n),\n",
    "$$\n",
    "under bigram model is computed as:\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2\\mid w_1)\\cdots P(w_n\\mid w_{n-1}),\n",
    "$$\n",
    "and under trigram model is computed as:\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2\\mid w_1)P(w_3\\mid w_1,w_2)\\cdots P(w_n\\mid w_{n-2},w_{n-1}),\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i find backhanded tweets/facebook comments to be the most immature thing ever .: unigram probability: 1.3924875341492407e-45\n",
      "i find backhanded tweets/facebook comments to be the most immature thing ever .: bigram probability: 6.79997948277606e-48\n",
      "i find backhanded tweets/facebook comments to be the most immature thing ever .: trigram probability: 1.1028096322476625e-58\n",
      "use coupon code thanksgiving !: unigram probability: 9.611075705568971e-19\n",
      "use coupon code thanksgiving !: bigram probability: 2.7083652741312752e-20\n",
      "use coupon code thanksgiving !: trigram probability: 2.4281651720929253e-23\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "from random import sample\n",
    "\n",
    "def uniprob(sen):\n",
    "    p = 1\n",
    "    for word in sen:\n",
    "        p *= calc_unigram_p(word)\n",
    "    return p\n",
    "\n",
    "def biprob(sen):\n",
    "    sentence = seq_ngrams(sen, 2)\n",
    "    p = calc_unigram_p(sen[0])\n",
    "    for bi in sentence:\n",
    "        p *= calc_bigram_p(bi[0], bi[1])\n",
    "    return p\n",
    "\n",
    "def triprob(sen):\n",
    "    sentence = seq_ngrams(sen, 3)\n",
    "    p = calc_unigram_p(sen[0]) * calc_bigram_p(sen[0], sen[1])\n",
    "    for bi in sentence:\n",
    "        p *= calc_trigram_p(bi[0], bi[1], bi[2])\n",
    "    return p\n",
    "\n",
    "# flag = 0\n",
    "# while(True):\n",
    "#     test = sample(test_data,2)\n",
    "#     for t in test:\n",
    "#         if len(t) < 4:\n",
    "#             flag = 1\n",
    "#             break\n",
    "#     if flag == 0:\n",
    "#         break\n",
    "#sampling\n",
    "test = sample(test_data,2)\n",
    "test = [[token for token in string if token not in punc]for string in test]\n",
    "#ð‘ƒ(ð‘¤1,â€¦,ð‘¤ð‘›)=ð‘ƒ(ð‘¤1)ð‘ƒ(ð‘¤2)â‹¯ð‘ƒ(ð‘¤ð‘›)\n",
    "for t in test:\n",
    "    print(f\"{' '.join(t)}: unigram probability: {uniprob(t)}\")\n",
    "    print(f\"{' '.join(t)}: bigram probability: {biprob(t)}\")\n",
    "    print(f\"{' '.join(t)}: trigram probability: {triprob(t)}\")\n",
    " \n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 (Finding The 3 Most Probable Next Words)\n",
    "In this part you are given a query including 2 words.  \n",
    "Query = i am  \n",
    "You need to find the 2 words that are more likely to be the Word that follows `am` under each language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram\n",
      "i am . !\n",
      "Bigram\n",
      "i am i 'm\n",
      "trigram\n",
      "i am so excited\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "from collections import Counter\n",
    "\n",
    "#unigram\n",
    "c1 = max(unigram_counts, key=unigram_counts.get)\n",
    "c2 = sorted(unigram_counts, key=unigram_counts.get)[-2] \n",
    "print('Unigram')\n",
    "print(f'i am {c1} {c2}')\n",
    "\n",
    "#bigram\n",
    "f = {}\n",
    "for verb in v:\n",
    "    f[verb] = calc_bigram_p('am', verb)\n",
    "x = Counter(f).most_common(1)\n",
    "g = {}\n",
    "for verb in v:\n",
    "    g[verb] = calc_bigram_p(x[0][0], verb)\n",
    "y = Counter(g).most_common(1)\n",
    "print('Bigram')\n",
    "print(f'i am {x[0][0]} {y[0][0]}')\n",
    "\n",
    "#trigram\n",
    "tr1 = {}\n",
    "for verb in v:\n",
    "    tr1[verb] = calc_trigram_p('i', 'am', verb)\n",
    "p = Counter(tr1).most_common(1)\n",
    "tr2 = {}\n",
    "for verb in v:\n",
    "    tr2[verb] = calc_trigram_p('am', p[0][0], verb)\n",
    "q = Counter(tr2).most_common(1)\n",
    "print('trigram')\n",
    "print(f'i am {p[0][0]} {q[0][0]}')\n",
    "### End your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e4e885df77660ca7e0b087d607dd7176d5ad4338e5d26cc40a96e95894b80be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
